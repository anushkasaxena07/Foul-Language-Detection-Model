{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea47169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Foul                                    Post language  Unnamed: 2\n",
      "6470   1.0                   y master hi chutiya h    hindi         NaN\n",
      "6471   1.0       kya lound master mila h behen cho    hindi         NaN\n",
      "6472   1.0  maa chudae master main to n krrha kaam    hindi         NaN\n",
      "6473   1.0                 baap thodi hu bsdk tera    hindi         NaN\n",
      "6474   1.0          ek baar baap bolde n apna bsdk    hindi         NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have loaded or created your English dataframe (df_en) and Hindi dataframe (df_hi)\n",
    "\n",
    "# Let's assume df_en and df_hi have the following structure:\n",
    "df_en = pd.read_csv('f2.csv')\n",
    "df_hi = pd.read_csv('f1.csv')\n",
    "\n",
    "# Now, let's add the 'language' column to each dataframe\n",
    "df_en['language'] = 'english'\n",
    "df_hi['language'] = 'hindi'\n",
    "\n",
    "# Concatenate the dataframes\n",
    "df_combined = pd.concat([df_en, df_hi], ignore_index=True)\n",
    "\n",
    "# Display the combined dataframe\n",
    "print(df_combined.tail())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3fec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the combined dataframe\n",
    "df_combined = df_combined.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb9fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (6475, 4)\n"
     ]
    }
   ],
   "source": [
    "# Save the combined dataframe to a CSV file\n",
    "df_combined.to_csv('multilingual_corpus.csv', index=False)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Replace 'your_dataset.csv' with the actual file path or dataset name\n",
    "df = pd.read_csv('multilingual_corpus.csv')\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00398b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\braje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the NLTK stopwords list\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5f78602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Set the NLTK data path\n",
    "nltk.data.path.append(\"C:\\\\nltk_data\")\n",
    "\n",
    "# Download the NLTK resources (including stopwords)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load spaCy model for Hindi\n",
    "nlp_hi = spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "def tokenize_and_preprocess(text, language):\n",
    "    # Tokenization\n",
    "    if language == 'english':\n",
    "        tokens = word_tokenize(text)\n",
    "    elif language == 'hindi':\n",
    "        tokens = [token.text for token in nlp_hi(text)]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token.isalnum()]\n",
    "\n",
    "    # Stemming or lemmatization (choose one)\n",
    "    stemmer_en = PorterStemmer()\n",
    "    stemmer_hi = SnowballStemmer('english')  # Fixed the typo here\n",
    "\n",
    "    if language == 'english':\n",
    "        tokens = [stemmer_en.stem(token) for token in tokens]\n",
    "    elif language == 'hindi':\n",
    "        tokens = [stemmer_hi.stem(token) for token in tokens]\n",
    "\n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Example usage:\n",
    "combined_corpus = [\n",
    "    {'text': 'This is an English sentence.', 'language': 'english'},\n",
    "    {'text': 'यह हिंदी वाक्य है।', 'language': 'hindi'}  # Fixed the language code\n",
    "]\n",
    "\n",
    "preprocessed_corpus = []\n",
    "\n",
    "for entry in combined_corpus:\n",
    "    tokens = tokenize_and_preprocess(entry['text'], entry['language'])\n",
    "    preprocessed_corpus.append({'tokens': tokens, 'language': entry['language']})\n",
    "\n",
    "# Display the preprocessed corpus\n",
    "for entry in preprocessed_corpus:\n",
    "    print(f\"Language: {entry['language']}, Tokens: {entry['tokens']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "179f260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #data analysis and processing lib #conv in row and colm(data frame)\n",
    "import numpy as np #for multidiamension array\n",
    "from sklearn.feature_extraction.text import CountVectorizer#nlp #extract features in the form support by ml algo#transform a given text into a vector on the basis of the frequency (count) of each word\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier #A decision tree classifier is used to solve classification problems, where the goal is to predict the class that a set of features should fall into 1. For example, it can be used to predict whether a person’s loan will be approved2.\n",
    "#On the other hand, a decision tree regressor is used to solve regression problems, where the goal is to predict an output variable that is continuous1. For instance, it can be used to predict the price of a house based on its features2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b6fe79a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re #regular expressions: match=re.search(pattern,text)\n",
    "import nltk #NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces,long with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "#NLP Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate speech. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
    "#Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.util import pr #objective of pr is to streamline the debugging process by allowing developers to implement complex print formatting with minimal effort.\n",
    "#pr()=print(\"\\n\"),pr(\"Hello world\", t=1, la=1, lb=1)\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(\"english\")# shortcuts cared ----> care university ----> univers  \n",
    "# stemming is reducing a word to its base word or stem in such a way that the words of similar kind lie under a common stem. \n",
    "#It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.\n",
    "from nltk.corpus import stopwords #(remove) stop word is a commonly used word (such as “the”, “a”, “an”, “in”)\n",
    "import string #upper case, lower case\n",
    "stopwords = set(stopwords.words(\"english\")) #sub string is part of main string ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a0e6f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Foul                                               Post language\n",
      "0   0.0  15. \"Arthvyavastha ka ubharna mushkil, shunya ...    hindi\n",
      "1   1.0     RT @ClicquotSuave: drop that pussy bitchhhhhhh  english\n",
      "2   1.0  RT @iH8TvvitterHoes: This was the most racist ...  english\n",
      "3   1.0  Gettin bitches was never hard I just settled d...  english\n",
      "4   1.0  @LiamGeraldShone @AranNicolWHUFC @DSaend ur a ...  english\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"multilingual_corpus.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29351cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Foul</th>\n",
       "      <th>Post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15. \"Arthvyavastha ka ubharna mushkil, shunya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @ClicquotSuave: drop that pussy bitchhhhhhh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @iH8TvvitterHoes: This was the most racist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Gettin bitches was never hard I just settled d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@LiamGeraldShone @AranNicolWHUFC @DSaend ur a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Foul                                               Post\n",
       "0   0.0  15. \"Arthvyavastha ka ubharna mushkil, shunya ...\n",
       "1   1.0     RT @ClicquotSuave: drop that pussy bitchhhhhhh\n",
       "2   1.0  RT @iH8TvvitterHoes: This was the most racist ...\n",
       "3   1.0  Gettin bitches was never hard I just settled d...\n",
       "4   1.0  @LiamGeraldShone @AranNicolWHUFC @DSaend ur a ..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[['Foul','Post']]\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ab7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Use r before the pattern string for a raw string\n",
    "    text = re.sub(r'http?://\\s+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)  # Replace newline characters with a space\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # Split the text into words and filter out stopwords\n",
    "    words = [word for word in text.split(' ') if word not in stopwords.words(\"english\")]\n",
    "    words = [word for word in text.split(' ') if word not in stopwords.words(\"Hinglish\")]\n",
    "    \n",
    "    # Apply stemming to each word\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join the stemmed words back together into a single string\n",
    "    cleaned_text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Assuming you have a DataFrame named 'df' with a column 'tweet'\n",
    "df[\"Post\"] = df[\"Post\"].apply(clean)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93bdfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df, np, and CountVectorizer are already imported and df is your DataFrame\n",
    "\n",
    "# Drop NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract features (x) and labels (y)\n",
    "x = np.array(df['Post'])\n",
    "y = np.array(df['Foul'])\n",
    "\n",
    "# Vectorize the text data\n",
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(x)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# Use RandomForestClassifier instead of DecisionTreeClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Now you can use the trained model for prediction\n",
    "predictions = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e264e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = \"fuck\"\n",
    "df = cv.transform([test_data]).toarray()\n",
    "if clf.predict(df) == 1:\n",
    "    print(\"Foul Language Detected\")\n",
    "else:\n",
    "    print(\"Clean Speech\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.externals import joblib\n",
    "# Assuming clf is your trained RandomForestClassifier\n",
    "# Save the model to a file\n",
    "joblib.dump(model, 'random_forest_model.joblib')\n",
    "\n",
    "# Now, you can load the model later using:\n",
    "# loaded_model = joblib.load('random_forest_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "md=joblib.load('random_forest_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6872ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
