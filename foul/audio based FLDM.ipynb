{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea47169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Foul                                    Post language  Unnamed: 2\n",
      "6470   1.0                   y master hi chutiya h    hindi         NaN\n",
      "6471   1.0       kya lound master mila h behen cho    hindi         NaN\n",
      "6472   1.0  maa chudae master main to n krrha kaam    hindi         NaN\n",
      "6473   1.0                 baap thodi hu bsdk tera    hindi         NaN\n",
      "6474   1.0          ek baar baap bolde n apna bsdk    hindi         NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have loaded or created your English dataframe (df_en) and Hindi dataframe (df_hi)\n",
    "\n",
    "# Let's assume df_en and df_hi have the following structure:\n",
    "df_en = pd.read_csv('f2.csv')\n",
    "df_hi = pd.read_csv('f1.csv')\n",
    "\n",
    "# Now, let's add the 'language' column to each dataframe\n",
    "df_en['language'] = 'english'\n",
    "df_hi['language'] = 'hindi'\n",
    "\n",
    "# Concatenate the dataframes\n",
    "df_combined = pd.concat([df_en, df_hi], ignore_index=True)\n",
    "\n",
    "# Display the combined dataframe\n",
    "print(df_combined.tail())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3fec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the combined dataframe\n",
    "df_combined = df_combined.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb9fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (6475, 4)\n"
     ]
    }
   ],
   "source": [
    "# Save the combined dataframe to a CSV file\n",
    "df_combined.to_csv('multilingual_corpus.csv', index=False)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Replace 'your_dataset.csv' with the actual file path or dataset name\n",
    "df = pd.read_csv('multilingual_corpus.csv')\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00398b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\braje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the NLTK stopwords list\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5f78602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Set the NLTK data path\n",
    "nltk.data.path.append(\"C:\\\\nltk_data\")\n",
    "\n",
    "# Download the NLTK resources (including stopwords)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "319d70e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\braje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\braje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'tokens' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m preprocessed_corpus \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m combined_corpus:\n\u001b[1;32m---> 53\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenize_and_preprocess(entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     54\u001b[0m     preprocessed_corpus\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m: tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m: entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Display the preprocessed corpus\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m, in \u001b[0;36mtokenize_and_preprocess\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[0;32m     24\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(language))\n\u001b[1;32m---> 25\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Remove special characters, numbers, and punctuation\u001b[39;00m\n\u001b[0;32m     28\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misalnum()]\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'tokens' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load spaCy model for Hindi\n",
    "nlp_hi = spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "def tokenize_and_preprocess(text, language):\n",
    "    # Tokenization\n",
    "    if language == 'english':\n",
    "        tokens = word_tokenize(text)\n",
    "    elif language == 'hindi':\n",
    "        tokens = [token.text for token in nlp_hi(text)]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token.isalnum()]\n",
    "\n",
    "    # Stemming or lemmatization (choose one)\n",
    "    stemmer_en = PorterStemmer()\n",
    "    stemmer_hi = SnowballStemmer('english')  # Fixed the typo here\n",
    "\n",
    "    if language == 'english':\n",
    "        tokens = [stemmer_en.stem(token) for token in tokens]\n",
    "    elif language == 'hindi':\n",
    "        tokens = [stemmer_hi.stem(token) for token in tokens]\n",
    "\n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Example usage:\n",
    "combined_corpus = [\n",
    "    {'text': 'This is an English sentence.', 'language': 'english'},\n",
    "    {'text': 'यह हिंदी वाक्य है।', 'language': 'hinglish'}  # Fixed the language code\n",
    "]\n",
    "\n",
    "preprocessed_corpus = []\n",
    "\n",
    "for entry in combined_corpus:\n",
    "    tokens = tokenize_and_preprocess(entry['text'], entry['language'])\n",
    "    preprocessed_corpus.append({'tokens': tokens, 'language': entry['language']})\n",
    "\n",
    "# Display the preprocessed corpus\n",
    "for entry in preprocessed_corpus:\n",
    "    print(f\"Language: {entry['language']}, Tokens: {entry['tokens']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "858c7784",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'tokens' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken3\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Call the function with some text\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m process_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a sample text.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m, in \u001b[0;36mprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_text\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Trying to access 'tokens' before assignment\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokens)  \u001b[38;5;66;03m# This will raise an UnboundLocalError\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Assuming 'tokens' is meant to be a local variable\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenize_text(text)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'tokens' where it is not associated with a value"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "179f260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #data analysis and processing lib #conv in row and colm(data frame)\n",
    "import numpy as np #for multidiamension array\n",
    "from sklearn.feature_extraction.text import CountVectorizer#nlp #extract features in the form support by ml algo#transform a given text into a vector on the basis of the frequency (count) of each word\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier #A decision tree classifier is used to solve classification problems, where the goal is to predict the class that a set of features should fall into 1. For example, it can be used to predict whether a person’s loan will be approved2.\n",
    "#On the other hand, a decision tree regressor is used to solve regression problems, where the goal is to predict an output variable that is continuous1. For instance, it can be used to predict the price of a house based on its features2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6fe79a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\braje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re #regular expressions: match=re.search(pattern,text)\n",
    "import nltk #NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces,long with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "#NLP Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate speech. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
    "#Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.util import pr #objective of pr is to streamline the debugging process by allowing developers to implement complex print formatting with minimal effort.\n",
    "#pr()=print(\"\\n\"),pr(\"Hello world\", t=1, la=1, lb=1)\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(\"english\")# shortcuts cared ----> care university ----> univers  \n",
    "# stemming is reducing a word to its base word or stem in such a way that the words of similar kind lie under a common stem. \n",
    "#It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.\n",
    "from nltk.corpus import stopwords #(remove) stop word is a commonly used word (such as “the”, “a”, “an”, “in”)\n",
    "import string #upper case, lower case\n",
    "stopwords = set(stopwords.words(\"english\")) #sub string is part of main string ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a0e6f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Foul                                               Post language  \\\n",
      "0   1.0  RT @killaaakam_: Who's gassin these hoes, BP? ...  english   \n",
      "1   1.0                                 Kamzor lund ka mut    hindi   \n",
      "2   1.0                                Kamzor lund ka muth    hindi   \n",
      "3   1.0                                     Tatti ke tiile    hindi   \n",
      "4   1.0                    I treat my bitches with respect  english   \n",
      "\n",
      "   Unnamed: 2  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"multilingual_corpus.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29351cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Foul</th>\n",
       "      <th>Post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @killaaakam_: Who's gassin these hoes, BP? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Kamzor lund ka mut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Kamzor lund ka muth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Tatti ke tiile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I treat my bitches with respect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Foul                                               Post\n",
       "0   1.0  RT @killaaakam_: Who's gassin these hoes, BP? ...\n",
       "1   1.0                                 Kamzor lund ka mut\n",
       "2   1.0                                Kamzor lund ka muth\n",
       "3   1.0                                     Tatti ke tiile\n",
       "4   1.0                    I treat my bitches with respect"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[['Foul','Post']]\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "541ab7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\braje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Foul                               Post\n",
      "0   1.0  rt killaaakam whos gassin hoe bp \n",
      "1   1.0                    kamzor lund mut\n",
      "2   1.0                   kamzor lund muth\n",
      "3   1.0                         tatti tiil\n",
      "4   1.0                treat bitch respect\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Use r before the pattern string for a raw string\n",
    "    text = re.sub(r'http?://\\s+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)  # Replace newline characters with a space\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # Split the text into words and filter out stopwords\n",
    "    words = [word for word in text.split(' ') if word not in stopwords.words(\"english\")]\n",
    "    words = [word for word in text.split(' ') if word not in stopwords.words(\"Hinglish\")]\n",
    "    \n",
    "    # Apply stemming to each word\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join the stemmed words back together into a single string\n",
    "    cleaned_text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Assuming you have a DataFrame named 'df' with a column 'tweet'\n",
    "df[\"Post\"] = df[\"Post\"].apply(clean)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c93bdfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.913312693498452\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df, np, and CountVectorizer are already imported and df is your DataFrame\n",
    "\n",
    "# Drop NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract features (x) and labels (y)\n",
    "x = np.array(df['Post'])\n",
    "y = np.array(df['Foul'])\n",
    "\n",
    "# Vectorize the text data\n",
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(x)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# Use RandomForestClassifier instead of DecisionTreeClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Now you can use the trained model for prediction\n",
    "predictions = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e264e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foul Language Detected\n"
     ]
    }
   ],
   "source": [
    "test_data = \"fuck\"\n",
    "df = cv.transform([test_data]).toarray()\n",
    "if clf.predict(df) == 1:\n",
    "    print(\"Foul Language Detected\")\n",
    "else:\n",
    "    print(\"Clean Speech\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfa1cd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vocabulary.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import joblib\n",
    "# Assuming clf is your trained RandomForestClassifier\n",
    "# Save the model to a file\n",
    "joblib.dump(clf,'foul_detector_model.joblib')\n",
    "joblib.dump(cv.vocabulary_, \"vocabulary.joblib\")\n",
    "# Now, you can load the model later using:\n",
    "# loaded_model = joblib.load('random_forest_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a01c5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "md=joblib.load('foul_detector_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6872ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(cv.vocabulary_, \"vocabulary.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
